{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python389jvsc74a57bd0303f4f94c3b299c215f3066923e10ebd3683e42f3be4d2a62ff11651779c3d00",
   "display_name": "Python 3.8.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "303f4f94c3b299c215f3066923e10ebd3683e42f3be4d2a62ff11651779c3d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# 0. Progressbar basics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          |0/10\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(total=len(range(10)), leave=True, bar_format='{l_bar}{bar}|{n_fmt}/{total_fmt}')\n",
    "\n",
    "for i in range(10):\n",
    "    pbar.update(n=1)"
   ]
  },
  {
   "source": [
    "# 1. Basic Custom training on Synthetic Data.\n",
    "### Define Model\n",
    "\n",
    "You define your model as a class. \n",
    "- `x` is your input tensor. \n",
    "- The model should output values of **wx+b**. \n",
    "- You'll start off by initializing w and b to random values. \n",
    "- During the training process, values of w and b get updated in accordance with linear regression so as to minimize the loss incurred by the model. \n",
    "- Once you arrive at optimal values for w and b, the model would have been trained to correctly predict the values of wx+b.\n",
    "\n",
    "Hence, \n",
    "- **w** and **b** are trainable weights of the model. \n",
    "- **x** is the input\n",
    "- **y** = wx + b is the output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.w = tf.Variable(2.0)\n",
    "        self.b = tf.Variable(1.0)\n",
    "    def __call__(self, x):\n",
    "        return self.w*x + self.b"
   ]
  },
  {
   "source": [
    "### Define a loss function\n",
    "\n",
    "A loss function measures how well the output of a model for a given input matches the target output. \n",
    "- The goal is to minimize this difference during training. \n",
    "- Let's use the standard L2 loss, also known as the least square errors\n",
    "$$Loss = \\sum_{i} \\left (y_{pred}^i - y_{target}^i \\right )^2$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred-y_true))"
   ]
  },
  {
   "source": [
    "### Obtain training data\n",
    "\n",
    "First, synthesize the training data using the \"true\" w and \"true\" b. \n",
    "\n",
    "$$y = w_{true} \\times x + b_{true} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w, true_b = 3.0, 2.0\n",
    "num_examples = 1000\n",
    "xs = tf.random.normal(shape=[num_examples])\n",
    "ys = true_w*xs + true_b"
   ]
  },
  {
   "source": [
    "### Define a training loop\n",
    "\n",
    "With the network and training data, train the model using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) \n",
    "- Gradient descent updates the trainable weights **w** and **b** to reduce the loss. \n",
    "\n",
    "\n",
    "There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer`—our recommended implementation. In the spirit of building from first principles, here you will implement the basic math yourself.\n",
    "- You'll use `tf.GradientTape` for automatic differentiation\n",
    "- Use `tf.assign_sub` for decrementing a value.  Note that assign_sub combines `tf.assign` and `tf.sub`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, lr):\n",
    "    with tf.GradientTape() as tape:\n",
    "        curr_loss = loss(model(inputs), outputs)\n",
    "    dw, db = tape.gradient(curr_loss, [model.w, model.b])\n",
    "    \n",
    "    model.w.assign_sub(lr*dw)\n",
    "    model.b.assign_sub(lr*db)\n",
    "    return curr_loss"
   ]
  },
  {
   "source": [
    "Finally, you can iteratively run through the training data and see how `w` and `b` evolve."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch=0, w=2.00, b=1.00, loss=1.8225\nEpoch=1, w=2.17, b=1.19, loss=1.2169\nEpoch=2, w=2.31, b=1.35, loss=0.8132\nEpoch=3, w=2.43, b=1.47, loss=0.5439\nEpoch=4, w=2.53, b=1.58, loss=0.3640\nEpoch=5, w=2.61, b=1.66, loss=0.2438\nEpoch=6, w=2.68, b=1.72, loss=0.1634\nEpoch=7, w=2.73, b=1.78, loss=0.1096\nEpoch=8, w=2.78, b=1.82, loss=0.0735\nEpoch=9, w=2.82, b=1.85, loss=0.0494\nEpoch=10, w=2.85, b=1.88, loss=0.0332\nEpoch=11, w=2.87, b=1.90, loss=0.0223\nEpoch=12, w=2.90, b=1.92, loss=0.0150\nEpoch=13, w=2.91, b=1.94, loss=0.0101\nEpoch=14, w=2.93, b=1.95, loss=0.0068\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "list_w, list_b = [], []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(15):\n",
    "    list_w.append(model.w.numpy())\n",
    "    list_b.append(model.b.numpy())\n",
    "    current_loss = train(model, inputs=xs, outputs=ys, lr=0.1)\n",
    "    losses.append(current_loss)\n",
    "    print(f'Epoch={epoch}, w={list_w[epoch]:.2f}, b={list_b[epoch]:.2f}, loss={losses[epoch]:.4f}')"
   ]
  },
  {
   "source": [
    "# 2. Custom training on fashion-mnist dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "45it [00:35,  1.25it/s]               \n",
      "  0%|          | 0/10 [00:24<?, ?it/s]\n",
      "45it [00:11,  3.84it/s]               \n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "LENGTH = 10 # Number of iterations required to fill pbar\n",
    "\n",
    "pbar = tqdm(total=LENGTH) # Init pbar\n",
    "for i in range(LENGTH):\n",
    "    pbar.update(n=1) # Increments counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  2%|▏         | 201405/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▍         | 478114/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 742951/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|█         | 1030937/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 1321184/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 1595004/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▉        | 1894756/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██▏       | 2196621/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▍       | 2498983/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 2826373/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███▏      | 3125993/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███▍      | 3392251/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 3679149/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▉      | 3946610/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 4259375/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 4549578/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 4831958/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 5113723/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 54%|█████▍    | 5401211/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 5680361/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|█████▉    | 5961662/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 6262975/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 6547446/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 6841328/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████▏  | 7142442/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|███████▍  | 7447986/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 7738420/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 8008925/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 8305202/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|████████▌ | 8614458/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 8914667/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 9224783/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▌| 9516338/10000000\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 10000000/10000000\n",
      "\n",
      "70000010it [03:16, 3893462.81it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0,int(10E6)), leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=100)\n",
    "for i in range(10):\n",
    "    pbar.update(10) \n",
    "    # n  : int Increment to add to the internal counter of iterations\n",
    "pbar.close()"
   ]
  },
  {
   "source": [
    "## Load and Preprocess Data\n",
    "You will load the [Fashion MNIST](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) dataset using Tensorflow Datasets. This dataset has 28 x 28 grayscale images of articles of clothing belonging to 10 clases.\n",
    "\n",
    "Here you are going to use the training and testing splits of the data. Testing split will be used for validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Now, we need to normalaize the data, as the data is greyscaled, ranged between (0,255). It will be faster to keep the range between (0,1).\n",
    "\n",
    "- We normalize the images by dividing them by 255.0 so as to make the pixels fall in the range (0, 1). You also reshape the data so as to flatten the 28 x 28 pixel array into a flattened 784 pixel array."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now you shuffle and batch your training and test datasets before feeding them to the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define the Model\n",
    "You are using a simple model in this example. You use Keras Functional API to connect two dense layers. The final layer is a softmax that outputs one of the 10 classes since this is a multi class classification problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define Optimizer and Loss Function\n",
    "\n",
    "You have chosen `adam` optimizer and sparse categorical crossentropy loss for this example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Define Metrics\n",
    "\n",
    "You will also define metrics so that your training loop can update and display them. Here you are using `SparseCategoricalAccuracy`defined in `tf.keras.metrics` since the problem at hand is a multi class classification problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building Training Loop\n",
    "In this section you build your training loop consisting of training and validation sequences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The core of training is using the model to calculate the logits on specific set of inputs and compute loss (in this case **sparse categorical crossentropy**) by comparing the predicted outputs to the true outputs. You then update the trainable weights using the optimizer algorithm chosen. Optimizer algorithm requires your computed loss and partial derivatives of loss with respect to each of the trainable weights to make updates to the same.\n",
    "\n",
    "You use gradient tape to calculate the gradients and then update the model trainable weights using the optimizer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}